{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "#from testCases_v4a import *\n",
    "#from dnn_utils_v2 import sigmoid, sigmoid_backward, relu, relu_backward\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    Implements the sigmoid activation in numpy\n",
    "    \n",
    "    Arguments:\n",
    "    Z -- numpy array of any shape\n",
    "    \n",
    "    Returns:\n",
    "    A -- output of sigmoid(z), same shape as Z\n",
    "    cache -- returns Z as well, useful during backpropagation\n",
    "    \"\"\"\n",
    "    \n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    Implement the RELU function.\n",
    "\n",
    "    Arguments:\n",
    "    Z -- Output of the linear layer, of any shape\n",
    "\n",
    "    Returns:\n",
    "    A -- Post-activation parameter, of the same shape as Z\n",
    "    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    A = np.maximum(0,Z)\n",
    "    \n",
    "    assert(A.shape == Z.shape)\n",
    "    \n",
    "    cache = Z \n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single RELU unit.\n",
    "\n",
    "    Arguments:\n",
    "    dA -- post-activation gradient, of any shape\n",
    "    cache -- 'Z' where we store for computing backward propagation efficiently\n",
    "\n",
    "    Returns:\n",
    "    dZ -- Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n",
    "    \n",
    "    # When z <= 0, you should set dz to 0 as well. \n",
    "    dZ[Z <= 0] = 0\n",
    "    \n",
    "    assert (dZ.shape == Z.shape)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coursera Grading Functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward_test_case():\n",
    "    np.random.seed(1)\n",
    "    \"\"\"\n",
    "    X = np.array([[-1.02387576, 1.12397796],\n",
    " [-1.62328545, 0.64667545],\n",
    " [-1.74314104, -0.59664964]])\n",
    "    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])\n",
    "    b = np.array([[1]])\n",
    "    \"\"\"\n",
    "    A = np.random.randn(3,2)\n",
    "    W = np.random.randn(1,3)\n",
    "    b = np.random.randn(1,1)\n",
    "    \n",
    "    return A, W, b\n",
    "\n",
    "def linear_activation_forward_test_case():\n",
    "    \"\"\"\n",
    "    X = np.array([[-1.02387576, 1.12397796],\n",
    " [-1.62328545, 0.64667545],\n",
    " [-1.74314104, -0.59664964]])\n",
    "    W = np.array([[ 0.74505627, 1.97611078, -1.24412333]])\n",
    "    b = 5\n",
    "    \"\"\"\n",
    "    np.random.seed(2)\n",
    "    A_prev = np.random.randn(3,2)\n",
    "    W = np.random.randn(1,3)\n",
    "    b = np.random.randn(1,1)\n",
    "    return A_prev, W, b\n",
    "\n",
    "def L_model_forward_test_case_2hidden():\n",
    "    np.random.seed(6)\n",
    "    X = np.random.randn(5,4)\n",
    "    W1 = np.random.randn(4,5)\n",
    "    b1 = np.random.randn(4,1)\n",
    "    W2 = np.random.randn(3,4)\n",
    "    b2 = np.random.randn(3,1)\n",
    "    W3 = np.random.randn(1,3)\n",
    "    b3 = np.random.randn(1,1)\n",
    "  \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2,\n",
    "                  \"W3\": W3,\n",
    "                  \"b3\": b3}\n",
    "    \n",
    "    return X, parameters\n",
    "\n",
    "def compute_cost_test_case():\n",
    "    Y = np.asarray([[1, 1, 0]])\n",
    "    aL = np.array([[.8,.9,0.4]])\n",
    "    \n",
    "    return Y, aL\n",
    "\n",
    "def linear_backward_test_case():\n",
    "    \"\"\"\n",
    "    z, linear_cache = (np.array([[-0.8019545 ,  3.85763489]]), (np.array([[-1.02387576,  1.12397796],\n",
    "       [-1.62328545,  0.64667545],\n",
    "       [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), np.array([[1]]))\n",
    "    \"\"\"\n",
    "    np.random.seed(1)\n",
    "    dZ = np.random.randn(3,4)\n",
    "    A = np.random.randn(5,4)\n",
    "    W = np.random.randn(3,5)\n",
    "    b = np.random.randn(3,1)\n",
    "    linear_cache = (A, W, b)\n",
    "    return dZ, linear_cache\n",
    "\n",
    "def linear_activation_backward_test_case():\n",
    "    \"\"\"\n",
    "    aL, linear_activation_cache = (np.array([[ 3.1980455 ,  7.85763489]]), ((np.array([[-1.02387576,  1.12397796], [-1.62328545,  0.64667545], [-1.74314104, -0.59664964]]), np.array([[ 0.74505627,  1.97611078, -1.24412333]]), 5), np.array([[ 3.1980455 ,  7.85763489]])))\n",
    "    \"\"\"\n",
    "    np.random.seed(2)\n",
    "    dA = np.random.randn(1,2)\n",
    "    A = np.random.randn(3,2)\n",
    "    W = np.random.randn(1,3)\n",
    "    b = np.random.randn(1,1)\n",
    "    Z = np.random.randn(1,2)\n",
    "    linear_cache = (A, W, b)\n",
    "    activation_cache = Z\n",
    "    linear_activation_cache = (linear_cache, activation_cache)\n",
    "    \n",
    "    return dA, linear_activation_cache\n",
    "\n",
    "def L_model_backward_test_case():\n",
    "    \"\"\"\n",
    "    X = np.random.rand(3,2)\n",
    "    Y = np.array([[1, 1]])\n",
    "    parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747]]), 'b1': np.array([[ 0.]])}\n",
    "\n",
    "    aL, caches = (np.array([[ 0.60298372,  0.87182628]]), [((np.array([[ 0.20445225,  0.87811744],\n",
    "           [ 0.02738759,  0.67046751],\n",
    "           [ 0.4173048 ,  0.55868983]]),\n",
    "    np.array([[ 1.78862847,  0.43650985,  0.09649747]]),\n",
    "    np.array([[ 0.]])),\n",
    "   np.array([[ 0.41791293,  1.91720367]]))])\n",
    "   \"\"\"\n",
    "    np.random.seed(3)\n",
    "    AL = np.random.randn(1, 2)\n",
    "    Y = np.array([[1, 0]])\n",
    "\n",
    "    A1 = np.random.randn(4,2)\n",
    "    W1 = np.random.randn(3,4)\n",
    "    b1 = np.random.randn(3,1)\n",
    "    Z1 = np.random.randn(3,2)\n",
    "    linear_cache_activation_1 = ((A1, W1, b1), Z1)\n",
    "\n",
    "    A2 = np.random.randn(3,2)\n",
    "    W2 = np.random.randn(1,3)\n",
    "    b2 = np.random.randn(1,1)\n",
    "    Z2 = np.random.randn(1,2)\n",
    "    linear_cache_activation_2 = ((A2, W2, b2), Z2)\n",
    "\n",
    "    caches = (linear_cache_activation_1, linear_cache_activation_2)\n",
    "\n",
    "    return AL, Y, caches\n",
    "\n",
    "def print_grads(grads):\n",
    "    print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "    print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "    print (\"dA1 = \"+ str(grads[\"dA1\"]))\n",
    "\n",
    "def update_parameters_test_case():\n",
    "    \"\"\"\n",
    "    parameters = {'W1': np.array([[ 1.78862847,  0.43650985,  0.09649747],\n",
    "        [-1.8634927 , -0.2773882 , -0.35475898],\n",
    "        [-0.08274148, -0.62700068, -0.04381817],\n",
    "        [-0.47721803, -1.31386475,  0.88462238]]),\n",
    " 'W2': np.array([[ 0.88131804,  1.70957306,  0.05003364, -0.40467741],\n",
    "        [-0.54535995, -1.54647732,  0.98236743, -1.10106763],\n",
    "        [-1.18504653, -0.2056499 ,  1.48614836,  0.23671627]]),\n",
    " 'W3': np.array([[-1.02378514, -0.7129932 ,  0.62524497],\n",
    "        [-0.16051336, -0.76883635, -0.23003072]]),\n",
    " 'b1': np.array([[ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.]]),\n",
    " 'b2': np.array([[ 0.],\n",
    "        [ 0.],\n",
    "        [ 0.]]),\n",
    " 'b3': np.array([[ 0.],\n",
    "        [ 0.]])}\n",
    "    grads = {'dW1': np.array([[ 0.63070583,  0.66482653,  0.18308507],\n",
    "        [ 0.        ,  0.        ,  0.        ],\n",
    "        [ 0.        ,  0.        ,  0.        ],\n",
    "        [ 0.        ,  0.        ,  0.        ]]),\n",
    " 'dW2': np.array([[ 1.62934255,  0.        ,  0.        ,  0.        ],\n",
    "        [ 0.        ,  0.        ,  0.        ,  0.        ],\n",
    "        [ 0.        ,  0.        ,  0.        ,  0.        ]]),\n",
    " 'dW3': np.array([[-1.40260776,  0.        ,  0.        ]]),\n",
    " 'da1': np.array([[ 0.70760786,  0.65063504],\n",
    "        [ 0.17268975,  0.15878569],\n",
    "        [ 0.03817582,  0.03510211]]),\n",
    " 'da2': np.array([[ 0.39561478,  0.36376198],\n",
    "        [ 0.7674101 ,  0.70562233],\n",
    "        [ 0.0224596 ,  0.02065127],\n",
    "        [-0.18165561, -0.16702967]]),\n",
    " 'da3': np.array([[ 0.44888991,  0.41274769],\n",
    "        [ 0.31261975,  0.28744927],\n",
    "        [-0.27414557, -0.25207283]]),\n",
    " 'db1': 0.75937676204411464,\n",
    " 'db2': 0.86163759922811056,\n",
    " 'db3': -0.84161956022334572}\n",
    "    \"\"\"\n",
    "    np.random.seed(2)\n",
    "    W1 = np.random.randn(3,4)\n",
    "    b1 = np.random.randn(3,1)\n",
    "    W2 = np.random.randn(1,3)\n",
    "    b2 = np.random.randn(1,1)\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    np.random.seed(3)\n",
    "    dW1 = np.random.randn(3,4)\n",
    "    db1 = np.random.randn(3,1)\n",
    "    dW2 = np.random.randn(1,3)\n",
    "    db2 = np.random.randn(1,1)\n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return parameters, grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing Parameters for NN :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters\n",
    "\n",
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    W1 = np.random.randn(n_h, n_x)*0.01\n",
    "    b1 = np.zeros((n_h, 1))\n",
    "    W2 = np.random.randn(n_y, n_h)*0.01\n",
    "    b2 = np.zeros((n_y, 1))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(W1.shape == (n_h, n_x))\n",
    "    assert(b1.shape == (n_h, 1))\n",
    "    assert(W2.shape == (n_y, n_h))\n",
    "    assert(b2.shape == (n_y, 1))\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01624345 -0.00611756 -0.00528172]\n",
      " [-0.01072969  0.00865408 -0.02301539]]\n",
      "b1 = [[0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.01744812 -0.00761207]]\n",
      "b2 = [[0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters(3,2,1)\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: initialize_parameters_deep\n",
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
    "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
    "\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[ 0.01788628  0.0043651   0.00096497 -0.01863493 -0.00277388]\n",
      " [-0.00354759 -0.00082741 -0.00627001 -0.00043818 -0.00477218]\n",
      " [-0.01313865  0.00884622  0.00881318  0.01709573  0.00050034]\n",
      " [-0.00404677 -0.0054536  -0.01546477  0.00982367 -0.01101068]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[-0.01185047 -0.0020565   0.01486148  0.00236716]\n",
      " [-0.01023785 -0.00712993  0.00625245 -0.00160513]\n",
      " [-0.00768836 -0.00230031  0.00745056  0.01976111]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([5,4,3])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_forward\n",
    "\n",
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implement the linear part of a layer's forward propagation.\n",
    "\n",
    "    Arguments:\n",
    "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- the input of the activation function, also called pre-activation parameter \n",
    "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    Z = np.dot(W, A)+b\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
    "    cache = (A, W, b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z = [[ 3.26295337 -1.23429987]]\n"
     ]
    }
   ],
   "source": [
    "A, W, b = linear_forward_test_case()\n",
    "\n",
    "Z, linear_cache = linear_forward(A, W, b)\n",
    "print(\"Z = \" + str(Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_forward\n",
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
    "\n",
    "    Arguments:\n",
    "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
    "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
    "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "\n",
    "    Returns:\n",
    "    A -- the output of the activation function, also called the post-activation value \n",
    "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
    "             stored for computing the backward pass efficiently\n",
    "    \"\"\"\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    elif activation == \"relu\":\n",
    "        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "        A, activation_cache = relu(Z)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
    "    cache = (linear_cache, activation_cache)\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_forward\n",
    "\n",
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (input size, number of examples)\n",
    "    parameters -- output of initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL -- last post-activation value\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
    "    \"\"\"\n",
    "\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters) // 2                  # number of layers in the neural network\n",
    "    #print(\"L= \", L)\n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n",
    "    for l in range(1, L):\n",
    "        #print(\"l= \", l)\n",
    "        A_prev = A \n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "        ### END CODE HERE ###\n",
    "    #print(\"l out= \", l)\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n",
    "    ### START CODE HERE ### (≈ 2 lines of code)\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(l+1)], parameters[\"b\"+str(l+1)], \"sigmoid\") #use A instaed of A_prev as it has been updated. Use (l+1) instead of (l) as loop ended on (L-1) and we need the value at L\n",
    "    caches.append(cache)\n",
    "    ### END CODE HERE ###\n",
    "    #print(AL.shape, \" \" , (1,X.shape[1]))\n",
    "    assert(AL.shape == (1,X.shape[1]))\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.03921668 0.70498921 0.19734387 0.04728177]]\n",
      "Length of caches list = 3\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case_2hidden()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Functions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: compute_cost\n",
    "\n",
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function defined by equation (7).\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
    "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1]\n",
    "\n",
    "    # Compute loss from aL and y.\n",
    "    ### START CODE HERE ### (≈ 1 lines of code)\n",
    "    cost = (-1/m)*(np.dot(Y, np.log(AL).T) + np.dot((1-Y), np.log(1-AL).T)) # dont screw up the brackets\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
    "    assert(cost.shape == ())\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cost = 0.2797765635793422\n"
     ]
    }
   ],
   "source": [
    "Y, AL = compute_cost_test_case()\n",
    "\n",
    "print(\"cost = \" + str(compute_cost(AL, Y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward Propagation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_backward\n",
    "\n",
    "def linear_backward(dZ, cache): #where did we calculate dZ? ... go to the next function for it 😊\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    dW = (1/m)*(np.dot(dZ, A_prev.T))\n",
    "    db = (1/m)*(np.sum(dZ, axis=1, keepdims=True))\n",
    "    dA_prev = np.dot(W.T, dZ)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[-1.15171336  0.06718465 -0.3204696   2.09812712]\n",
      " [ 0.60345879 -3.72508701  5.81700741 -3.84326836]\n",
      " [-0.4319552  -1.30987417  1.72354705  0.05070578]\n",
      " [-0.38981415  0.60811244 -1.25938424  1.47191593]\n",
      " [-2.52214926  2.67882552 -0.67947465  1.48119548]]\n",
      "dW = [[ 0.07313866 -0.0976715  -0.87585828  0.73763362  0.00785716]\n",
      " [ 0.85508818  0.37530413 -0.59912655  0.71278189 -0.58931808]\n",
      " [ 0.97913304 -0.24376494 -0.08839671  0.55151192 -0.10290907]]\n",
      "db = [[-0.14713786]\n",
      " [-0.11313155]\n",
      " [-0.13209101]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: linear_activation_backward\n",
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    Arguments:\n",
    "    dA -- post-activation gradient for current layer l \n",
    "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
    "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "    elif activation == \"sigmoid\":\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "dAL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(dAL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL <------------WHY?\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    ### START CODE HERE ### (1 line of code)\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    ### START CODE HERE ### (approx. 2 lines)\n",
    "    current_cache = caches[L-1]\n",
    "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, activation = \"sigmoid\")\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop from l=L-2 to l=0\n",
    "    for l in reversed(range(L-1)): #<------------how does reversed work? why from L-2?\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        ### START CODE HERE ### (approx. 5 lines)\n",
    "        current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l+1)], current_cache, activation = \"relu\")\n",
    "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.12913162 -0.44014127]\n",
      " [-0.14175655  0.48317296]\n",
      " [ 0.01663708 -0.05670698]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print_grads(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: update_parameters\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "                  parameters[\"W\" + str(l)] = ... \n",
    "                  parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    # Update rule for each parameter. Use a for loop.\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*(grads[\"dW\" + str(l+1)])\n",
    "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*(grads[\"db\" + str(l+1)])\n",
    "    ### END CODE HERE ###\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import scipy\n",
    "from PIL import Image\n",
    "from scipy import ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x_orig, train_y, test_x_orig, test_y, classes = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y = 0. It's a non-cat picture.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of a picture\n",
    "index = 10\n",
    "plt.imshow(train_x_orig[index])\n",
    "print (\"y = \" + str(train_y[0,index]) + \". It's a \" + classes[train_y[0,index]].decode(\"utf-8\") +  \" picture.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 209\n",
      "Number of testing examples: 50\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_x_orig shape: (209, 64, 64, 3)\n",
      "train_y shape: (1, 209)\n",
      "test_x_orig shape: (50, 64, 64, 3)\n",
      "test_y shape: (1, 50)\n"
     ]
    }
   ],
   "source": [
    "# Explore your dataset \n",
    "m_train = train_x_orig.shape[0]\n",
    "num_px = train_x_orig.shape[1]\n",
    "m_test = test_x_orig.shape[0]\n",
    "\n",
    "print (\"Number of training examples: \" + str(m_train))\n",
    "print (\"Number of testing examples: \" + str(m_test))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_x_orig shape: \" + str(train_x_orig.shape))\n",
    "print (\"train_y shape: \" + str(train_y.shape))\n",
    "print (\"test_x_orig shape: \" + str(test_x_orig.shape))\n",
    "print (\"test_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x's shape: (12288, 209)\n",
      "test_x's shape: (12288, 50)\n"
     ]
    }
   ],
   "source": [
    "# Reshape the training and test examples \n",
    "train_x_flatten = train_x_orig.reshape(train_x_orig.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n",
    "test_x_flatten = test_x_orig.reshape(test_x_orig.shape[0], -1).T\n",
    "\n",
    "# Standardize data to have feature values between 0 and 1.\n",
    "train_x = train_x_flatten/255.\n",
    "test_x = test_x_flatten/255.\n",
    "\n",
    "print (\"train_x's shape: \" + str(train_x.shape))\n",
    "print (\"test_x's shape: \" + str(test_x.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS DEFINING THE MODEL ####\n",
    "n_x = 12288     # num_px * num_px * 3\n",
    "n_h = 7\n",
    "n_y = 1\n",
    "layers_dims = (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: two_layer_model \n",
    "\n",
    "#<--------------------- might have used deep NN funtions in 2-layer NN ---------------------------->\n",
    "\n",
    "def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a two-layer neural network: LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- dimensions of the layers (n_x, n_h, n_y)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- If set to True, this will print the cost every 100 iterations \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- a dictionary containing W1, W2, b1, and b2\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(1)\n",
    "    grads = {}\n",
    "    costs = []                              # to keep track of the cost\n",
    "    m = X.shape[1]                           # number of examples\n",
    "    (n_x, n_h, n_y) = layers_dims\n",
    "    \n",
    "    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n",
    "    ### START CODE HERE ### (≈ 1 line of code)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y) \n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Get W1, b1, W2 and b2 from the dictionary parameters.\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        A1, cache1 = linear_activation_forward(X, W1, b1, activation=\"relu\")\n",
    "        A2, cache2 = linear_activation_forward(A1, W2, b2, activation=\"sigmoid\")\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(A2, Y)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Initializing backward propagation\n",
    "        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n",
    "        \n",
    "        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n",
    "        ### START CODE HERE ### (≈ 2 lines of code)\n",
    "        dA1, dW2, db2 = linear_activation_backward(dA2, cache2, activation = \"sigmoid\")\n",
    "        dA0, dW1, db1 = linear_activation_backward(dA1, cache1, activation = \"relu\")\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n",
    "        grads['dW1'] = dW1\n",
    "        grads['db1'] = db1\n",
    "        grads['dW2'] = dW2\n",
    "        grads['db2'] = db2\n",
    "        \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (approx. 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "        # Retrieve W1, b1, W2, b2 from parameters\n",
    "        W1 = parameters[\"W1\"]\n",
    "        b1 = parameters[\"b1\"]\n",
    "        W2 = parameters[\"W2\"]\n",
    "        b2 = parameters[\"b2\"]\n",
    "        \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "       \n",
    "    # plot the cost\n",
    "\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6930497356599888\n",
      "Cost after iteration 100: 0.6464320953428849\n",
      "Cost after iteration 200: 0.6325140647912677\n",
      "Cost after iteration 300: 0.6015024920354665\n",
      "Cost after iteration 400: 0.5601966311605748\n",
      "Cost after iteration 500: 0.5158304772764729\n",
      "Cost after iteration 600: 0.47549013139433255\n",
      "Cost after iteration 700: 0.4339163151225749\n",
      "Cost after iteration 800: 0.400797753620389\n",
      "Cost after iteration 900: 0.3580705011323798\n",
      "Cost after iteration 1000: 0.3394281538366412\n",
      "Cost after iteration 1100: 0.3052753636196263\n",
      "Cost after iteration 1200: 0.27491377282130197\n",
      "Cost after iteration 1300: 0.24681768210614846\n",
      "Cost after iteration 1400: 0.19850735037466088\n",
      "Cost after iteration 1500: 0.17448318112556663\n",
      "Cost after iteration 1600: 0.17080762978096237\n",
      "Cost after iteration 1700: 0.11306524562164721\n",
      "Cost after iteration 1800: 0.09629426845937147\n",
      "Cost after iteration 1900: 0.08342617959726861\n",
      "Cost after iteration 2000: 0.07439078704319078\n",
      "Cost after iteration 2100: 0.06630748132267926\n",
      "Cost after iteration 2200: 0.059193295010381654\n",
      "Cost after iteration 2300: 0.05336140348560552\n",
      "Cost after iteration 2400: 0.04855478562877016\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = two_layer_model(train_x, train_y, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, y, parameters):\n",
    "    \"\"\"\n",
    "    This function is used to predict the results of a  L-layer neural network.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data set of examples you would like to label\n",
    "    parameters -- parameters of the trained model\n",
    "    \n",
    "    Returns:\n",
    "    p -- predictions for the given dataset X\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[1]\n",
    "    n = len(parameters) // 2 # number of layers in the neural network\n",
    "    p = np.zeros((1,m))\n",
    "    \n",
    "    # Forward propagation\n",
    "    probas, caches = L_model_forward(X, parameters)\n",
    "\n",
    "    \n",
    "    # convert probas to 0/1 predictions\n",
    "    for i in range(0, probas.shape[1]):\n",
    "        if probas[0,i] > 0.5:\n",
    "            p[0,i] = 1\n",
    "        else:\n",
    "            p[0,i] = 0\n",
    "    \n",
    "    #print results\n",
    "    #print (\"predictions: \" + str(p))\n",
    "    #print (\"true labels: \" + str(y))\n",
    "    print(\"Accuracy: \"  + str(np.sum((p == y)/m)))\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9999999999999998\n"
     ]
    }
   ],
   "source": [
    "predictions_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.72\n"
     ]
    }
   ],
   "source": [
    "predictions_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONSTANTS ###\n",
    "layers_dims = [12288, 20, 7, 5, 1] #  4-layer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_layer_model\n",
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization. (≈ 1 line of code)\n",
    "    ### START CODE HERE ###\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        # Compute cost.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        cost = compute_cost(AL, Y)\n",
    "        ### END CODE HERE ###\n",
    "    \n",
    "        # Backward propagation.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        ### END CODE HERE ###\n",
    " \n",
    "        # Update parameters.\n",
    "        ### START CODE HERE ### (≈ 1 line of code)\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        ### END CODE HERE ###\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693148\n",
      "Cost after iteration 100: 0.678011\n",
      "Cost after iteration 200: 0.667600\n",
      "Cost after iteration 300: 0.660422\n",
      "Cost after iteration 400: 0.655458\n",
      "Cost after iteration 500: 0.652013\n",
      "Cost after iteration 600: 0.649616\n",
      "Cost after iteration 700: 0.647942\n",
      "Cost after iteration 800: 0.646770\n",
      "Cost after iteration 900: 0.645947\n",
      "Cost after iteration 1000: 0.645368\n",
      "Cost after iteration 1100: 0.644961\n",
      "Cost after iteration 1200: 0.644673\n",
      "Cost after iteration 1300: 0.644469\n",
      "Cost after iteration 1400: 0.644325\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = L_layer_model(train_x, train_y, layers_dims, num_iterations = 1500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6555023923444976\n"
     ]
    }
   ],
   "source": [
    "pred_train = predict(train_x, train_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3400000000000001\n"
     ]
    }
   ],
   "source": [
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'print_mislabeled_images' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-152-76d78ca5a144>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint_mislabeled_images\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_y\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpred_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'print_mislabeled_images' is not defined"
     ]
    }
   ],
   "source": [
    "print_mislabeled_images(classes, test_x, test_y, pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## START CODE HERE ##\n",
    "my_image = \"my_image.jpg\" # change this to the name of your image file \n",
    "my_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\n",
    "## END CODE HERE ##\n",
    "\n",
    "fname = \"images/\" + my_image\n",
    "image = np.array(ndimage.imread(fname, flatten=False))\n",
    "my_image = scipy.misc.imresize(image, size=(num_px,num_px)).reshape((num_px*num_px*3,1))\n",
    "my_image = my_image/255.\n",
    "my_predicted_image = predict(my_image, my_label_y, parameters)\n",
    "\n",
    "plt.imshow(image)\n",
    "print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
